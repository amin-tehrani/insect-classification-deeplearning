{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f4e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hajibagher/insect-classification-deeplearning'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e9b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchviz\n",
    "\n",
    "import vit\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import dnaencoder\n",
    "import pandas as pd\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mat import mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791296d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b3c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba81798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: amintehrani/dnaencoder-insects-finetuned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a20bd399ded4c288c383f5e359873a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4861fd6cf4294474bcdad5b29640fdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0329dbaed44eb1a714922774727a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffb8356d4374fe287af1251cb6704d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee35730a6c74fc1aecdd9c57096f82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "esm_config.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/amintehrani/dnaencoder-insects-finetuned:\n",
      "- esm_config.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a072e6c21f42c48842b613c43cc28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_esm.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/amintehrani/dnaencoder-insects-finetuned:\n",
      "- modeling_esm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mdict\u001b[39m(\u001b[43mdnaencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamintehrani/dnaencoder-insects-finetuned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/insect-classification-deeplearning/dnaencoder.py:212\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(mat, model_name, device)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# config = AutoConfig.from_pretrained(model_name)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[1;32m    211\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 212\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(load_file(join(model_name, \"model.safetensors\")))\u001b[39;00m\n\u001b[1;32m    216\u001b[0m dna_strings \u001b[38;5;241m=\u001b[39m mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_string_dnas\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:593\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m         model_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class(auto_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    592\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    597\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:316\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:4910\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4901\u001b[0m     gguf_file\n\u001b[1;32m   4902\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4903\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4904\u001b[0m ):\n\u001b[1;32m   4905\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4906\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4908\u001b[0m     )\n\u001b[0;32m-> 4910\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4928\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4930\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4931\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:1178\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   1177\u001b[0m     }\n\u001b[0;32m-> 1178\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:321\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    264\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    265\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    267\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         snapshot_download(\n\u001b[1;32m    495\u001b[0m             path_or_repo_id,\n\u001b[1;32m    496\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    506\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1171\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1738\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n\u001b[1;32m   1732\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1736\u001b[0m             )\n\u001b[0;32m-> 1738\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1748\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:426\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file is too large to be downloaded using the regular download method. Use `hf_transfer` or `hf_xet` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Try `pip install hf_transfer` or `pip install hf_xet`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         )\n\u001b[0;32m--> 426\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    431\u001b[0m total: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_file_length_from_http_response(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m429\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    706\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m     62\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:974\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# and socket type values to enum constants.\u001b[39;00m\n\u001b[1;32m    973\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 974\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    975\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    976\u001b[0m     addrlist\u001b[38;5;241m.\u001b[39mappend((_intenum_converter(af, AddressFamily),\n\u001b[1;32m    977\u001b[0m                      _intenum_converter(socktype, SocketKind),\n\u001b[1;32m    978\u001b[0m                      proto, canonname, sa))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict(dnaencoder.evaluate_model(mat, \"amintehrani/dnaencoder-insects-finetuned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb0a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='233' max='233' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [233/233 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('val_seen_indices',\n",
       "  {'eval_loss': 0.82353675365448,\n",
       "   'eval_model_preparation_time': 0.0026,\n",
       "   'eval_accuracy': 0.8528138528138528,\n",
       "   'eval_runtime': 16.6445,\n",
       "   'eval_samples_per_second': 194.299,\n",
       "   'eval_steps_per_second': 6.128}),\n",
       " ('val_unseen_indices',\n",
       "  {'eval_loss': 10.643092155456543,\n",
       "   'eval_model_preparation_time': 0.003,\n",
       "   'eval_accuracy': 0.013705993012631014,\n",
       "   'eval_runtime': 18.0735,\n",
       "   'eval_samples_per_second': 205.882,\n",
       "   'eval_steps_per_second': 6.474}),\n",
       " ('test_seen_indices',\n",
       "  {'eval_loss': 2.636476755142212,\n",
       "   'eval_model_preparation_time': 0.0029,\n",
       "   'eval_accuracy': 0.6951903807615231,\n",
       "   'eval_runtime': 24.294,\n",
       "   'eval_samples_per_second': 205.401,\n",
       "   'eval_steps_per_second': 6.421}),\n",
       " ('test_unseen_indices',\n",
       "  {'eval_loss': 10.732027053833008,\n",
       "   'eval_model_preparation_time': 0.0029,\n",
       "   'eval_accuracy': 0.01599462365591398,\n",
       "   'eval_runtime': 36.441,\n",
       "   'eval_samples_per_second': 204.166,\n",
       "   'eval_steps_per_second': 6.394})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"amintehrani/vit-insects-finetuned7\"\n",
    "dict(vit.evaluate_model(mat, model_name, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670983f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13039 3234 3721 4990 7440\n"
     ]
    }
   ],
   "source": [
    "all_images = mat['all_images']\n",
    "all_labels = mat['all_labels'].squeeze()\n",
    "train_indices = mat['train_loc'].squeeze()\n",
    "val_seen_indices = mat['val_seen_loc'].squeeze()\n",
    "val_unseen_indices = mat['val_unseen_loc'].squeeze()\n",
    "test_seen_indices = mat['test_seen_loc'].squeeze()\n",
    "test_unseen_indices = mat['test_unseen_loc'].squeeze()\n",
    "\n",
    "val_indices = np.concatenate((val_seen_indices, val_unseen_indices))\n",
    "test_indices = np.concatenate((test_seen_indices, test_unseen_indices))\n",
    "\n",
    "total_images = all_images.shape[0]\n",
    "print(train_indices.shape[0], val_seen_indices.shape[0], val_unseen_indices.shape[0], test_seen_indices.shape[0], test_unseen_indices.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7e518c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3234)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"val_seen_loc\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ff19ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: batch_dna_0.pkl\n",
      "Loading file: batch_dna_1.pkl\n",
      "Loading file: batch_dna_2.pkl\n",
      "Loading file: batch_dna_3.pkl\n",
      "Loading file: batch_dna_4.pkl\n",
      "Loading file: batch_dna_5.pkl\n",
      "Loading file: batch_dna_6.pkl\n",
      "Loading file: batch_dna_7.pkl\n",
      "Loading file: batch_dna_8.pkl\n",
      "Loading file: batch_dna_9.pkl\n",
      "Loading file: batch_dna_10.pkl\n",
      "Loading file: batch_dna_11.pkl\n",
      "Loading file: batch_dna_12.pkl\n",
      "Loading file: batch_dna_13.pkl\n",
      "Loading file: batch_dna_14.pkl\n",
      "Loading file: batch_dna_15.pkl\n",
      "Loading file: batch_dna_16.pkl\n",
      "Loading file: batch_dna_17.pkl\n",
      "Loading file: batch_dna_18.pkl\n",
      "Loading file: batch_dna_19.pkl\n",
      "Loading file: batch_dna_20.pkl\n",
      "Loading file: batch_dna_21.pkl\n",
      "Loading file: batch_dna_22.pkl\n",
      "Loading file: batch_dna_23.pkl\n",
      "Loading file: batch_dna_24.pkl\n",
      "Loading file: batch_dna_25.pkl\n",
      "Loading file: batch_dna_26.pkl\n",
      "Loading file: batch_dna_27.pkl\n",
      "Loading file: batch_dna_28.pkl\n",
      "Loading file: batch_dna_29.pkl\n",
      "Loading file: batch_dna_30.pkl\n",
      "Loading file: batch_dna_31.pkl\n",
      "Loading file: batch_dna_32.pkl\n",
      "Loading file: batch_dna_33.pkl\n",
      "Loading file: batch_dna_34.pkl\n",
      "Loading file: batch_dna_35.pkl\n",
      "Loading file: batch_dna_36.pkl\n",
      "Loading file: batch_dna_37.pkl\n",
      "Loading file: batch_dna_38.pkl\n",
      "Loading file: batch_dna_39.pkl\n",
      "Loading file: batch_dna_40.pkl\n",
      "Loading file: batch_dna_41.pkl\n",
      "Loading file: batch_dna_42.pkl\n",
      "Loading file: batch_dna_43.pkl\n",
      "Loading file: batch_dna_44.pkl\n",
      "Loading file: batch_dna_45.pkl\n",
      "Loading file: batch_dna_46.pkl\n",
      "Loading file: batch_dna_47.pkl\n",
      "Loading file: batch_dna_48.pkl\n",
      "Loading file: batch_dna_49.pkl\n",
      "Loading file: batch_dna_50.pkl\n",
      "Loading file: batch_dna_51.pkl\n",
      "Loading file: batch_dna_52.pkl\n",
      "Loading file: batch_dna_53.pkl\n",
      "Loading file: batch_dna_54.pkl\n",
      "Loading file: batch_dna_55.pkl\n",
      "Loading file: batch_dna_56.pkl\n",
      "Loading file: batch_dna_57.pkl\n",
      "Loading file: batch_dna_58.pkl\n",
      "Loading file: batch_dna_59.pkl\n",
      "Loading file: batch_dna_60.pkl\n",
      "Loading file: batch_dna_61.pkl\n",
      "Loading file: batch_dna_62.pkl\n",
      "Loading file: batch_dna_63.pkl\n",
      "Loading file: batch_dna_64.pkl\n",
      "Loading file: batch_dna_65.pkl\n",
      "Loading file: batch_dna_66.pkl\n",
      "Loading file: batch_dna_67.pkl\n",
      "Loading file: batch_dna_68.pkl\n",
      "Loading file: batch_dna_69.pkl\n",
      "Loading file: batch_dna_70.pkl\n",
      "Loading file: batch_dna_71.pkl\n",
      "Loading file: batch_dna_72.pkl\n",
      "Loading file: batch_dna_73.pkl\n",
      "Loading file: batch_dna_74.pkl\n",
      "Loading file: batch_dna_75.pkl\n",
      "Loading file: batch_dna_76.pkl\n",
      "Loading file: batch_dna_77.pkl\n",
      "Loading file: batch_dna_78.pkl\n",
      "Loading file: batch_dna_79.pkl\n",
      "Loading file: batch_dna_80.pkl\n",
      "Loading file: batch_dna_81.pkl\n",
      "Loading file: batch_dna_82.pkl\n",
      "Loading file: batch_dna_83.pkl\n",
      "Loading file: batch_dna_84.pkl\n",
      "Loading file: batch_dna_85.pkl\n",
      "Loading file: batch_dna_86.pkl\n",
      "Loading file: batch_dna_87.pkl\n",
      "Loading file: batch_dna_88.pkl\n",
      "Loading file: batch_dna_89.pkl\n",
      "Loading file: batch_dna_90.pkl\n",
      "Loading file: batch_dna_91.pkl\n",
      "Loading file: batch_dna_92.pkl\n",
      "Loading file: batch_dna_93.pkl\n",
      "Loading file: batch_dna_94.pkl\n",
      "Loading file: batch_dna_95.pkl\n",
      "Loading file: batch_dna_96.pkl\n",
      "Loading file: batch_dna_97.pkl\n",
      "Loading file: batch_dna_98.pkl\n",
      "Loading file: batch_dna_99.pkl\n",
      "Loading file: batch_dna_100.pkl\n",
      "Loading file: batch_dna_101.pkl\n",
      "Loading file: batch_dna_102.pkl\n",
      "Loading file: batch_dna_103.pkl\n",
      "Loading file: batch_dna_104.pkl\n",
      "Loading file: batch_dna_105.pkl\n",
      "Loading file: batch_dna_106.pkl\n",
      "Loading file: batch_dna_107.pkl\n",
      "Loading file: batch_dna_108.pkl\n",
      "Loading file: batch_dna_109.pkl\n",
      "Loading file: batch_dna_110.pkl\n",
      "Loading file: batch_dna_111.pkl\n",
      "Loading file: batch_dna_112.pkl\n",
      "Loading file: batch_dna_113.pkl\n",
      "Loading file: batch_dna_114.pkl\n",
      "Loading file: batch_dna_115.pkl\n",
      "Loading file: batch_dna_116.pkl\n",
      "Loading file: batch_dna_117.pkl\n",
      "Loading file: batch_dna_118.pkl\n",
      "Loading file: batch_dna_119.pkl\n",
      "Loading file: batch_dna_120.pkl\n",
      "Loading file: batch_dna_121.pkl\n",
      "Loading file: batch_dna_122.pkl\n",
      "Loading file: batch_dna_123.pkl\n",
      "Loading file: batch_dna_124.pkl\n",
      "Loading file: batch_dna_125.pkl\n",
      "Loading file: batch_dna_126.pkl\n",
      "Loading file: batch_dna_127.pkl\n",
      "Loading file: batch_dna_128.pkl\n",
      "Loading file: batch_dna_129.pkl\n",
      "Loading file: batch_dna_130.pkl\n",
      "Loading file: batch_dna_131.pkl\n",
      "Loading file: batch_dna_132.pkl\n",
      "Loading file: batch_dna_133.pkl\n",
      "Loading file: batch_dna_134.pkl\n",
      "Loading file: batch_dna_135.pkl\n",
      "Loading file: batch_dna_136.pkl\n",
      "Loading file: batch_dna_137.pkl\n",
      "Loading file: batch_dna_138.pkl\n",
      "Loading file: batch_dna_139.pkl\n",
      "Loading file: batch_dna_140.pkl\n",
      "Loading file: batch_dna_141.pkl\n",
      "Loading file: batch_dna_142.pkl\n",
      "Loading file: batch_dna_143.pkl\n",
      "Loading file: batch_dna_144.pkl\n",
      "Loading file: batch_dna_145.pkl\n",
      "Loading file: batch_dna_146.pkl\n",
      "Loading file: batch_dna_147.pkl\n",
      "Loading file: batch_dna_148.pkl\n",
      "Loading file: batch_dna_149.pkl\n",
      "Loading file: batch_dna_150.pkl\n",
      "Loading file: batch_dna_151.pkl\n",
      "Loading file: batch_dna_152.pkl\n",
      "Loading file: batch_dna_153.pkl\n",
      "Loading file: batch_dna_154.pkl\n",
      "Loading file: batch_dna_155.pkl\n",
      "Loading file: batch_dna_156.pkl\n",
      "Loading file: batch_dna_157.pkl\n",
      "Loading file: batch_dna_158.pkl\n",
      "Loading file: batch_dna_159.pkl\n",
      "Loading file: batch_dna_160.pkl\n",
      "Loading file: batch_dna_161.pkl\n",
      "Loading file: batch_dna_162.pkl\n",
      "Loading file: batch_dna_163.pkl\n",
      "Loading file: batch_dna_164.pkl\n",
      "Loading file: batch_dna_165.pkl\n",
      "Loading file: batch_dna_166.pkl\n",
      "Loading file: batch_dna_167.pkl\n",
      "Loading file: batch_dna_168.pkl\n",
      "Loading file: batch_dna_169.pkl\n",
      "Loading file: batch_dna_170.pkl\n",
      "Loading file: batch_dna_171.pkl\n",
      "Loading file: batch_dna_172.pkl\n",
      "Loading file: batch_dna_173.pkl\n",
      "Loading file: batch_dna_174.pkl\n",
      "Loading file: batch_dna_175.pkl\n",
      "Loading file: batch_dna_176.pkl\n",
      "Loading file: batch_dna_177.pkl\n",
      "Loading file: batch_dna_178.pkl\n",
      "Loading file: batch_dna_179.pkl\n",
      "Loading file: batch_dna_180.pkl\n",
      "Loading file: batch_dna_181.pkl\n",
      "Loading file: batch_dna_182.pkl\n",
      "Loading file: batch_dna_183.pkl\n",
      "Loading file: batch_dna_184.pkl\n",
      "Loading file: batch_dna_185.pkl\n",
      "Loading file: batch_dna_186.pkl\n",
      "Loading file: batch_dna_187.pkl\n",
      "Loading file: batch_dna_188.pkl\n",
      "Loading file: batch_dna_189.pkl\n",
      "Loading file: batch_dna_190.pkl\n",
      "Loading file: batch_dna_191.pkl\n",
      "Loading file: batch_dna_192.pkl\n",
      "Loading file: batch_dna_193.pkl\n",
      "Loading file: batch_dna_194.pkl\n",
      "Loading file: batch_dna_195.pkl\n",
      "Loading file: batch_dna_196.pkl\n",
      "Loading file: batch_dna_197.pkl\n",
      "Loading file: batch_dna_198.pkl\n",
      "Loading file: batch_dna_199.pkl\n",
      "Loading file: batch_dna_200.pkl\n",
      "Loading file: batch_dna_201.pkl\n",
      "Loading file: batch_dna_202.pkl\n",
      "Loading file: batch_dna_203.pkl\n",
      "Loading file: batch_dna_204.pkl\n",
      "Loading file: batch_dna_205.pkl\n",
      "Loading file: batch_dna_206.pkl\n",
      "Loading file: batch_dna_207.pkl\n",
      "Loading file: batch_dna_208.pkl\n",
      "Loading file: batch_dna_209.pkl\n",
      "Loading file: batch_dna_210.pkl\n",
      "Loading file: batch_dna_211.pkl\n",
      "Loading file: batch_dna_212.pkl\n",
      "Loading file: batch_dna_213.pkl\n",
      "Loading file: batch_dna_214.pkl\n",
      "Loading file: batch_dna_215.pkl\n",
      "Loading file: batch_dna_216.pkl\n",
      "Loading file: batch_dna_217.pkl\n",
      "Loading file: batch_dna_218.pkl\n",
      "Loading file: batch_dna_219.pkl\n",
      "Loading file: batch_dna_220.pkl\n",
      "Loading file: batch_dna_221.pkl\n",
      "Loading file: batch_dna_222.pkl\n",
      "Loading file: batch_dna_223.pkl\n",
      "Loading file: batch_dna_224.pkl\n",
      "Loading file: batch_dna_225.pkl\n",
      "Loading file: batch_dna_226.pkl\n",
      "Loading file: batch_dna_227.pkl\n",
      "Loading file: batch_dna_228.pkl\n",
      "Loading file: batch_dna_229.pkl\n",
      "Loading file: batch_dna_230.pkl\n",
      "Loading file: batch_dna_231.pkl\n",
      "Loading file: batch_dna_232.pkl\n",
      "Loading file: batch_dna_233.pkl\n",
      "Loading file: batch_dna_234.pkl\n",
      "Loading file: batch_dna_235.pkl\n",
      "Loading file: batch_dna_236.pkl\n",
      "Loading file: batch_dna_237.pkl\n",
      "Loading file: batch_dna_238.pkl\n",
      "Loading file: batch_dna_239.pkl\n",
      "Loading file: batch_dna_240.pkl\n",
      "Loading file: batch_dna_241.pkl\n",
      "Loading file: batch_dna_242.pkl\n",
      "Loading file: batch_dna_243.pkl\n",
      "Loading file: batch_dna_244.pkl\n",
      "Loading file: batch_dna_245.pkl\n",
      "Loading file: batch_dna_246.pkl\n",
      "Loading file: batch_dna_247.pkl\n",
      "Loading file: batch_dna_248.pkl\n",
      "Loading file: batch_dna_249.pkl\n",
      "Loading file: batch_dna_250.pkl\n",
      "Loading file: batch_dna_251.pkl\n",
      "Loading file: batch_dna_252.pkl\n",
      "Loading file: batch_dna_253.pkl\n",
      "Loading file: batch_img_0.pkl\n",
      "Loading file: batch_img_1.pkl\n",
      "Loading file: batch_img_2.pkl\n",
      "Loading file: batch_img_3.pkl\n",
      "Loading file: batch_img_4.pkl\n",
      "Loading file: batch_img_5.pkl\n",
      "Loading file: batch_img_6.pkl\n",
      "Loading file: batch_img_7.pkl\n",
      "Loading file: batch_img_8.pkl\n",
      "Loading file: batch_img_9.pkl\n",
      "Loading file: batch_img_10.pkl\n",
      "Loading file: batch_img_11.pkl\n",
      "Loading file: batch_img_12.pkl\n",
      "Loading file: batch_img_13.pkl\n",
      "Loading file: batch_img_14.pkl\n",
      "Loading file: batch_img_15.pkl\n",
      "Loading file: batch_img_16.pkl\n",
      "Loading file: batch_img_17.pkl\n",
      "Loading file: batch_img_18.pkl\n",
      "Loading file: batch_img_19.pkl\n",
      "Loading file: batch_img_20.pkl\n",
      "Loading file: batch_img_21.pkl\n",
      "Loading file: batch_img_22.pkl\n",
      "Loading file: batch_img_23.pkl\n",
      "Loading file: batch_img_24.pkl\n",
      "Loading file: batch_img_25.pkl\n",
      "Loading file: batch_img_26.pkl\n",
      "Loading file: batch_img_27.pkl\n",
      "Loading file: batch_img_28.pkl\n",
      "Loading file: batch_img_29.pkl\n",
      "Loading file: batch_img_30.pkl\n",
      "Loading file: batch_img_31.pkl\n",
      "Loading file: batch_img_32.pkl\n",
      "Loading file: batch_img_33.pkl\n",
      "Loading file: batch_img_34.pkl\n",
      "Loading file: batch_img_35.pkl\n",
      "Loading file: batch_img_36.pkl\n",
      "Loading file: batch_img_37.pkl\n",
      "Loading file: batch_img_38.pkl\n",
      "Loading file: batch_img_39.pkl\n",
      "Loading file: batch_img_40.pkl\n",
      "Loading file: batch_img_41.pkl\n",
      "Loading file: batch_img_42.pkl\n",
      "Loading file: batch_img_43.pkl\n",
      "Loading file: batch_img_44.pkl\n",
      "Loading file: batch_img_45.pkl\n",
      "Loading file: batch_img_46.pkl\n",
      "Loading file: batch_img_47.pkl\n",
      "Loading file: batch_img_48.pkl\n",
      "Loading file: batch_img_49.pkl\n",
      "Loading file: batch_img_50.pkl\n",
      "Loading file: batch_img_51.pkl\n",
      "Loading file: batch_img_52.pkl\n",
      "Loading file: batch_img_53.pkl\n",
      "Loading file: batch_img_54.pkl\n",
      "Loading file: batch_img_55.pkl\n",
      "Loading file: batch_img_56.pkl\n",
      "Loading file: batch_img_57.pkl\n",
      "Loading file: batch_img_58.pkl\n",
      "Loading file: batch_img_59.pkl\n",
      "Loading file: batch_img_60.pkl\n",
      "Loading file: batch_img_61.pkl\n",
      "Loading file: batch_img_62.pkl\n",
      "Loading file: batch_img_63.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_seen_loc': <multimodal_dataset.MultiModalDataset at 0x7fd5b620fc10>,\n",
       " 'val_unseen_loc': <multimodal_dataset.MultiModalDataset at 0x7fd5b5ee72d0>,\n",
       " 'test_seen_loc': <multimodal_dataset.MultiModalDataset at 0x7fd6c33c63d0>,\n",
       " 'test_unseen_loc': <multimodal_dataset.MultiModalDataset at 0x7fd6b2b4c190>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multimodal_dataset import MultiModalDataset\n",
    "from load_embeddings import load_dna_embeddings, load_img_embeddings\n",
    "\n",
    "all_dna_features = load_dna_embeddings()\n",
    "all_image_features = load_img_embeddings()\n",
    "\n",
    "all_dna_len = list(map(lambda s: len(s.strip()), mat['all_string_dnas']))\n",
    "dna_str_len_mapping: dict[int,int] = {}\n",
    "\n",
    "def dna_str_len_to_int(s_len):\n",
    "    if s_len not in dna_str_len_mapping:\n",
    "        dna_str_len_mapping[s_len] = len(dna_str_len_mapping)\n",
    "    return dna_str_len_mapping[s_len]\n",
    "\n",
    "# def all_dna_len_token():\n",
    "#     return list(map(dna_str_len_to_int, all_dna_len))\n",
    "\n",
    "all_dna_len_tokens = list(map(dna_str_len_to_int, all_dna_len))\n",
    "all_dna_len_tokens = np.array(all_dna_len_tokens, dtype=np.int64)\n",
    "\n",
    "species2genus = mat['species2genus']-1\n",
    "\n",
    "\n",
    "genus_species = dict()\n",
    "max_specie_in_genus = 0\n",
    "for genus_id, genus in pd.DataFrame(species2genus, columns=['genus']).groupby('genus'):\n",
    "    specie_indices = genus.index.tolist()\n",
    "    genus_species[genus_id] = specie_indices\n",
    "    if len(specie_indices) > max_specie_in_genus:\n",
    "        max_specie_in_genus = len(specie_indices)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    indicies_type:MultiModalDataset(\n",
    "        mat['all_string_dnas'][(inds:=mat[indicies_type].flatten())],\n",
    "        mat['all_images'][inds],\n",
    "        np.transpose(mat['all_labels'], (1,0))[inds],\n",
    "        dna_str_len_mapping,\n",
    "        species2genus,\n",
    "        genus_species,\n",
    "        None,\n",
    "        None,\n",
    "        dna_embeddings=all_dna_features[inds],\n",
    "        img_embeddings=all_image_features[inds]\n",
    "    )\n",
    "    for indicies_type in [\"val_seen_loc\", \"val_unseen_loc\", \"test_seen_loc\", \"test_unseen_loc\"]\n",
    "}\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e5010a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mdict\u001b[39m(\u001b[43mmain_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(datasets))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "dict(main_classifier.evaluate(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "557fe077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config: {'fusion_embedder': {'dna_dim': 512, 'img_dim': 768, 'fused_dim': 258, 'dna_len_dim': 32, 'num_distinct_dna_len': 120, 'proj_dna_dim': 96, 'proj_img_dim': 128, 'dropout': 0.2}, 'genus_classifier': {'fused_dim': 258, 'hidden_dim': 744, 'genus_n_classes': 372, 'dropout': 0.1, 'dna_len_dim': 32}, 'local_specie_classifier': {'fused_dim': 258, 'genus_n_classes': 372, 'reduced_fused_dim': 128, 'max_specie_in_genus': 23, 'genus_embedding_dim': 64, 'specie_decoder_hidden_dim': 256, 'dropout': 0.1, 'dna_len_dim': 32}}\n",
      "Loaded model state: <All keys matched successfully>\n",
      "Evaluating val_seen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating val_unseen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_seen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_unseen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_seen_loc': {'eval_loss': 15.198629379272461,\n",
       "  'eval_accuracy': 0.5355596784168213,\n",
       "  'eval_runtime': 2.1439,\n",
       "  'eval_samples_per_second': 1508.435,\n",
       "  'eval_steps_per_second': 23.788},\n",
       " 'val_unseen_loc': {'eval_loss': 19.26132583618164,\n",
       "  'eval_accuracy': 0.10131685030905671,\n",
       "  'eval_runtime': 2.4401,\n",
       "  'eval_samples_per_second': 1524.945,\n",
       "  'eval_steps_per_second': 24.179},\n",
       " 'test_seen_loc': {'eval_loss': 15.887377738952637,\n",
       "  'eval_accuracy': 0.43967935871743485,\n",
       "  'eval_runtime': 3.1852,\n",
       "  'eval_samples_per_second': 1566.637,\n",
       "  'eval_steps_per_second': 24.489},\n",
       " 'test_unseen_loc': {'eval_loss': 19.262758255004883,\n",
       "  'eval_accuracy': 0.12352150537634408,\n",
       "  'eval_runtime': 4.7665,\n",
       "  'eval_samples_per_second': 1560.893,\n",
       "  'eval_steps_per_second': 24.546}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate:\n",
    "import models\n",
    "reload(models)\n",
    "from models import AttentionFusion, GenusClassifier, LocalSpecieClassfier, MainClassifier, multimodal_collector\n",
    "\n",
    "fusion_embedder = AttentionFusion(dna_dim=512,img_dim=768,dna_len_dim=32, fused_dim=258, proj_dna_dim=128-32, proj_img_dim=128, dropout=0.2)\n",
    "# print(\"fusion_embedder\", fusion_embedder._config)\n",
    "# print(\"Fusion model created. fused dim: \", fusion_embedder.fused_dim)\n",
    "genus_classifier = GenusClassifier(fusion_embedder.fused_dim,dropout=0.1, dna_len_dim=32)\n",
    "local_specie_classifier = LocalSpecieClassfier(fusion_embedder.fused_dim,reduced_fused_dim=128, specie_decoder_hidden_dim=256, dropout=0.1,dna_len_dim=32)\n",
    "\n",
    "# main_classifier = MainClassifier(mat['species2genus'], genus_species, None, None, fusion_embedder, genus_classifier,local_specie_classifier,alpha=2, beta=0, theta=0,).to(device)\n",
    "main_classifier = MainClassifier.load_model(\"./results_20250827T234918-final\", mat['species2genus'], genus_species).to(device)\n",
    "\n",
    "assert main_classifier.fusion_embedder._config == fusion_embedder._config\n",
    "assert main_classifier.genus_classifier._config == genus_classifier._config\n",
    "assert main_classifier.local_specie_classifier._config == local_specie_classifier._config\n",
    "# print(main_classifier)\n",
    "dict(main_classifier.evaluate(datasets))\n",
    "\n",
    "\n",
    "# print(main_classifier)\n",
    "# main_classifier.load_model(\"results_20250819T030148-final\")\n",
    "# train_indices = (mat['train_loc'] - 1).flatten()  # Get train indices\n",
    "# train_dataset = MultiModalDataset(mat['all_string_dnas'][train_indices], mat['all_images'][train_indices], np.transpose(mat['all_labels'], (1,0))[train_indices], dna_str_len_mapping, species2genus, genus_species, None, None, dna_embeddings=all_dna_features[train_indices], img_embeddings=all_image_features[train_indices])\n",
    "# main_classifier.fit(\n",
    "#     train_dataset,\n",
    "#     None,\n",
    "#     batch_size=64,\n",
    "#     epochs=1,\n",
    "#     eval_steps=200,\n",
    "#     save_steps=400,\n",
    "#     lr=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1352eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_classifier = MainClassifier(mat['species2genus'], genus_species, None, None, None, None,local_specie_classifier,alpha=2, beta=0, theta=0,).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d0d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea82dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config: {'fusion_embedder': {'num_distinct_dna_len': 120, 'dna_len_dim': 32, 'fused_dim': 256, 'n_heads': 4, 'ff_mult': 4, 'dropout': 0.2, 'num_layers': 1}, 'genus_classifier': {'fused_dim': 256, 'hidden_dim': 744, 'genus_n_classes': 372, 'dropout': 0.2, 'dna_len_dim': 32}, 'local_specie_classifier': {'fused_dim': 256, 'genus_n_classes': 372, 'reduced_fused_dim': 128, 'max_specie_in_genus': 23, 'genus_embedding_dim': 64, 'specie_decoder_hidden_dim': 256, 'dropout': 0.2, 'dna_len_dim': 32}}\n",
      "Loaded model state: <All keys matched successfully>\n",
      "Evaluating val_seen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/26 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating val_unseen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_seen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_unseen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_seen_loc': {'eval_accuracy': 0.7845691382765531,\n",
      "                   'eval_loss': 20.35539436340332,\n",
      "                   'eval_runtime': 2.448,\n",
      "                   'eval_samples_per_second': 2038.365,\n",
      "                   'eval_steps_per_second': 15.931},\n",
      " 'test_unseen_loc': {'eval_accuracy': 0.11922043010752688,\n",
      "                     'eval_loss': 32.198673248291016,\n",
      "                     'eval_runtime': 3.6753,\n",
      "                     'eval_samples_per_second': 2024.313,\n",
      "                     'eval_steps_per_second': 16.053},\n",
      " 'val_seen_loc': {'eval_accuracy': 0.9329004329004329,\n",
      "                  'eval_loss': 17.840614318847656,\n",
      "                  'eval_runtime': 1.8435,\n",
      "                  'eval_samples_per_second': 1754.29,\n",
      "                  'eval_steps_per_second': 14.104},\n",
      " 'val_unseen_loc': {'eval_accuracy': 0.11341037355549584,\n",
      "                    'eval_loss': 32.35670471191406,\n",
      "                    'eval_runtime': 1.8621,\n",
      "                    'eval_samples_per_second': 1998.331,\n",
      "                    'eval_steps_per_second': 16.111}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main_classifier2 = MainClassifier.load_modelV2(\"./model_trained_v2_20250828T164138-final\", mat['species2genus'], genus_species).to(device)\n",
    "\n",
    "pprint(dict(main_classifier2.evaluate(datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b1392e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config: {'fusion_embedder': {'dna_dim': 512, 'img_dim': 768, 'fused_dim': 256, 'dna_len_dim': 32, 'num_distinct_dna_len': 120, 'proj_dna_dim': 96, 'proj_img_dim': 128, 'dropout': 0.2}, 'genus_classifier': {'fused_dim': 256, 'hidden_dim': 744, 'genus_n_classes': 372, 'dropout': 0.2, 'dna_len_dim': 32}, 'local_specie_classifier': {'fused_dim': 256, 'genus_n_classes': 372, 'reduced_fused_dim': 128, 'max_specie_in_genus': 23, 'genus_embedding_dim': 64, 'specie_decoder_hidden_dim': 256, 'dropout': 0.2, 'dna_len_dim': 32}}\n",
      "Loaded model state: <All keys matched successfully>\n",
      "Evaluating val_seen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/26 00:00 < 00:01, 17.21 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating val_unseen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_seen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_unseen_loc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['dna_emb'] = torch.tensor(self.dna_embeddings[idx], dtype=torch.float32)\n",
      "/home/hajibagher/insect-classification-deeplearning/multimodal_dataset.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['img_emb'] = torch.tensor(self.img_embeddings[idx], dtype=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_seen_loc': {'eval_accuracy': 0.7707414829659318,\n",
      "                   'eval_loss': 20.461692810058594,\n",
      "                   'eval_runtime': 2.4174,\n",
      "                   'eval_samples_per_second': 2064.229,\n",
      "                   'eval_steps_per_second': 16.133},\n",
      " 'test_unseen_loc': {'eval_accuracy': 0.10188172043010753,\n",
      "                     'eval_loss': 32.160709381103516,\n",
      "                     'eval_runtime': 3.3767,\n",
      "                     'eval_samples_per_second': 2203.326,\n",
      "                     'eval_steps_per_second': 17.473},\n",
      " 'val_seen_loc': {'eval_accuracy': 0.91713048855906,\n",
      "                  'eval_loss': 17.949588775634766,\n",
      "                  'eval_runtime': 1.5887,\n",
      "                  'eval_samples_per_second': 2035.688,\n",
      "                  'eval_steps_per_second': 16.366},\n",
      " 'val_unseen_loc': {'eval_accuracy': 0.07309862940069874,\n",
      "                    'eval_loss': 32.578216552734375,\n",
      "                    'eval_runtime': 1.8592,\n",
      "                    'eval_samples_per_second': 2001.407,\n",
      "                    'eval_steps_per_second': 16.136}}\n"
     ]
    }
   ],
   "source": [
    "main_classifier2 = MainClassifier.load_model(\"./model_trained_20250828T164930-final\", mat['species2genus'], genus_species).to(device)\n",
    "\n",
    "pprint(dict(main_classifier2.evaluate(datasets)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
