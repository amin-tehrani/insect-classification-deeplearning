{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58c9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amintehrani/.pyenv/versions/3.12.8/envs/torchgeo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchviz\n",
    "\n",
    "import vit\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from mat import mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6b6392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'all_images', 'all_dnas', 'all_labels', 'all_dnas_norepeat', 'all_dna_labels_norepeat', 'all_boldids', 'train_loc', 'val_seen_loc', 'val_unseen_loc', 'test_seen_loc', 'test_unseen_loc', 'species2genus', 'described_species_labels_train', 'described_species_labels_trainval', 'all_dna_features_cnn_original', 'all_image_features_resnet', 'all_image_features_gan', 'all_dna_features_cnn_new', 'all_string_dnas'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d8d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "Max specie in genus:  23\n"
     ]
    }
   ],
   "source": [
    "species2genus = mat['species2genus']-1\n",
    "\n",
    "# group species by genus\n",
    "\n",
    "genus_species = dict()\n",
    "max_specie_in_genus = 0\n",
    "for genus_id, genus in pd.DataFrame(species2genus, columns=['genus']).groupby('genus'):\n",
    "    specie_indices = genus.index.tolist()\n",
    "    genus_species[genus_id] = specie_indices\n",
    "    if len(specie_indices) > max_specie_in_genus:\n",
    "        max_specie_in_genus = len(specie_indices)\n",
    "\n",
    "print(len(genus_species))\n",
    "print(\"Max specie in genus: \", max_specie_in_genus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db67da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=512)\n",
    "all_dna_features_cnn_pca = np.array(pca.fit_transform(mat['all_dna_features_cnn_new']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23faf696",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=768)\n",
    "all_image_features_gan_pca = np.array(pca.fit_transform(mat['all_image_features_gan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1623ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_dna_len = list(map(lambda s: len(s.strip()), mat['all_string_dnas']))\n",
    "dna_str_len_mapping: dict[int,int] = {}\n",
    "\n",
    "def dna_str_len_to_int(s_len):\n",
    "    if s_len not in dna_str_len_mapping:\n",
    "        dna_str_len_mapping[s_len] = len(dna_str_len_mapping)\n",
    "    return dna_str_len_mapping[s_len]\n",
    "\n",
    "# def all_dna_len_token():\n",
    "#     return list(map(dna_str_len_to_int, all_dna_len))\n",
    "\n",
    "all_dna_len_tokens = list(map(dna_str_len_to_int, all_dna_len))\n",
    "all_dna_len_tokens = np.array(all_dna_len_tokens, dtype=np.int64)\n",
    "print(list(zip(all_dna_len, all_dna_len_tokens))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f79d14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deviceGPU = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "deviceCPU = torch.device(\"cpu\")\n",
    "\n",
    "device = deviceCPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb82502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at ./vit-finetuned7-final and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vit\n",
    "reload(vit)\n",
    "from vit import get_processor_encoder, get_img_embedding\n",
    "img_processor, img_encoder = get_processor_encoder(\"./vit-finetuned7-final\", device)\n",
    "get_img_embedding(mat['all_images'][:2], img_processor, img_encoder, device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4fbb029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dnaencoder\n",
    "reload(dnaencoder)\n",
    "from dnaencoder import get_tokenizer_encoder, get_dna_embedding\n",
    "dna_tokenizer, dna_encoder = get_tokenizer_encoder(\"./dnaencoder-finetuned1755100772-final\", deviceGPU)\n",
    "get_dna_embedding(mat['all_string_dnas'][:2], dna_tokenizer, dna_encoder).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513ccdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multimodal_dataset\n",
    "reload(multimodal_dataset)\n",
    "from multimodal_dataset import MultiModalDataset\n",
    "dataset = MultiModalDataset(mat['all_string_dnas'], mat['all_images'], np.transpose(mat['all_labels'], (1,0)), dna_str_len_mapping, species2genus, genus_species, None, None, \n",
    "                            dna_embeddings=all_dna_features_cnn_pca, img_embeddings=all_image_features_gan_pca)\n",
    "(dataset[0])\n",
    "_ = (dataset[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ae4d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32424, 512)\n"
     ]
    }
   ],
   "source": [
    "print(all_dna_features_cnn_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9285ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = (mat['train_loc'] - 1).flatten()  # Get train indices\n",
    "val_indices = (mat['val_seen_loc'] - 1).flatten()  # Get validation indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61d9bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multimodal_dataset\n",
    "reload(multimodal_dataset)\n",
    "from multimodal_dataset import MultiModalDataset\n",
    "# all_dataset = MultiModalDataset(mat['all_string_dnas'], mat['all_images'], np.transpose(mat['all_labels'], (1,0)), dna_str_len_mapping, species2genus, genus_species, None, None, \n",
    "#                             dna_embeddings=all_dna_features_cnn_pca, img_embeddings=all_image_features_gan_pca)\n",
    "\n",
    "train_dataset = MultiModalDataset(mat['all_string_dnas'][train_indices], mat['all_images'][train_indices], np.transpose(mat['all_labels'], (1,0))[train_indices], dna_str_len_mapping, species2genus, genus_species, None, None, dna_embeddings=all_dna_features_cnn_pca[train_indices], img_embeddings=all_image_features_gan_pca[train_indices])\n",
    "val_dataset = MultiModalDataset(mat['all_string_dnas'][val_indices], mat['all_images'][val_indices], np.transpose(mat['all_labels'], (1,0))[val_indices], dna_str_len_mapping, species2genus, genus_species, None, None, dna_embeddings=all_dna_features_cnn_pca[val_indices], img_embeddings=all_image_features_gan_pca[val_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc413f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion model created. fused dim:  1296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import models\n",
    "reload(models)\n",
    "from models import AttentionFusion, GenusClassifier, LocalSpecieClassfier, MainClassifier, multimodal_collector\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "\n",
    "def get_main_classifier():\n",
    "    fusion_embedder = AttentionFusion(dna_dim=512,img_dim=768,dna_len_dim=16)\n",
    "    print(\"Fusion model created. fused dim: \", fusion_embedder.fused_dim)\n",
    "    genus_classifier = GenusClassifier(fusion_embedder.fused_dim)\n",
    "\n",
    "    local_specie_classifier = LocalSpecieClassfier(fusion_embedder.fused_dim)\n",
    "\n",
    "    return MainClassifier(mat['species2genus'], genus_species, None, None, fusion_embedder, genus_classifier, local_specie_classifier).to(device)\n",
    "\n",
    "main_classifier = get_main_classifier()\n",
    "\n",
    "# print(main_classifier(**multimodal_collector([dataset[0], dataset[1]])))\n",
    "# print(main_classifier(**dataset[0]))\n",
    "# print(main_classifier(**dataset[0:2]))\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Disable Weights & Biases logging\n",
    "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"  # Disable Weights & Biases logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "368aabdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1224/1224 03:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.137100</td>\n",
       "      <td>5.111212</td>\n",
       "      <td>0.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.935100</td>\n",
       "      <td>4.928825</td>\n",
       "      <td>0.654298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.874200</td>\n",
       "      <td>4.876467</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7f59b4225ee0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_classifier.fit(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    eval_steps=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
