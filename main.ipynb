{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58c9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amintehrani/.pyenv/versions/3.12.8/envs/torchgeo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchviz\n",
    "\n",
    "import vit\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from mat import mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff6b6392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'all_images', 'all_dnas', 'all_labels', 'all_dnas_norepeat', 'all_dna_labels_norepeat', 'all_boldids', 'train_loc', 'val_seen_loc', 'val_unseen_loc', 'test_seen_loc', 'test_unseen_loc', 'species2genus', 'described_species_labels_train', 'described_species_labels_trainval', 'all_dna_features_cnn_original', 'all_image_features_resnet', 'all_image_features_gan', 'all_dna_features_cnn_new', 'all_string_dnas'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d8d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "Max specie in genus:  23\n"
     ]
    }
   ],
   "source": [
    "species2genus = mat['species2genus']-1\n",
    "\n",
    "# group species by genus\n",
    "\n",
    "genus_species = dict()\n",
    "max_specie_in_genus = 0\n",
    "for genus_id, genus in pd.DataFrame(species2genus, columns=['genus']).groupby('genus'):\n",
    "    specie_indices = genus.index.tolist()\n",
    "    genus_species[genus_id] = specie_indices\n",
    "    if len(specie_indices) > max_specie_in_genus:\n",
    "        max_specie_in_genus = len(specie_indices)\n",
    "\n",
    "print(len(genus_species))\n",
    "print(\"Max specie in genus: \", max_specie_in_genus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=512)\n",
    "# all_dna_features_cnn_pca = pca.fit_transform(mat['all_dna_features_cnn_new'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23faf696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=512)\n",
    "# all_image_features_gan_pca = pca.fit_transform(mat['all_image_features_gan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1623ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0)), (658, np.int64(0))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_dna_len = list(map(lambda s: len(s.strip()), mat['all_string_dnas']))\n",
    "dna_str_len_mapping: dict[int,int] = {}\n",
    "\n",
    "def dna_str_len_to_int(s_len):\n",
    "    if s_len not in dna_str_len_mapping:\n",
    "        dna_str_len_mapping[s_len] = len(dna_str_len_mapping)\n",
    "    return dna_str_len_mapping[s_len]\n",
    "\n",
    "# def all_dna_len_token():\n",
    "#     return list(map(dna_str_len_to_int, all_dna_len))\n",
    "\n",
    "all_dna_len_tokens = list(map(dna_str_len_to_int, all_dna_len))\n",
    "all_dna_len_tokens = np.array(all_dna_len_tokens, dtype=np.int64)\n",
    "print(list(zip(all_dna_len, all_dna_len_tokens))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f79d14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deviceGPU = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "deviceCPU = torch.device(\"cpu\")\n",
    "\n",
    "device = deviceCPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb82502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at ./vit-finetuned7-final and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vit\n",
    "reload(vit)\n",
    "from vit import get_processor_encoder, get_img_embedding\n",
    "img_processor, img_encoder = get_processor_encoder(\"./vit-finetuned7-final\", device)\n",
    "get_img_embedding(mat['all_images'][:2], img_processor, img_encoder, device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4fbb029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dnaencoder\n",
    "reload(dnaencoder)\n",
    "from dnaencoder import get_tokenizer_encoder, get_dna_embedding\n",
    "dna_tokenizer, dna_encoder = get_tokenizer_encoder(\"./dnaencoder-finetuned1755100772-final\", deviceGPU)\n",
    "get_dna_embedding(mat['all_string_dnas'][:2], dna_tokenizer, dna_encoder).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "513ccdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1600])\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "reload(dataset)\n",
    "from dataset import MultiModalDataset\n",
    "dataset = MultiModalDataset(mat['all_string_dnas'], mat['all_images'], np.transpose(mat['all_labels'], (1,0)), dna_str_len_mapping, species2genus, genus_species, img_processor, dna_tokenizer)\n",
    "print(dataset[2:5]['dna_inputs']['input_ids'].shape)\n",
    "# import models\n",
    "# reload(models)\n",
    "# from models import AttentionFusion, GenusClassifier, LocalSpecieClassfier, MainClassifier, multimodal_collator\n",
    "\n",
    "# fusion_embedder = AttentionFusion(dna_dim=512,img_dim=768,dna_len_dim=16)\n",
    "# print(\"Fusion model created. fused dim: \", fusion_embedder.fused_dim)\n",
    "# genus_classifier = GenusClassifier(fusion_embedder.fused_dim)\n",
    "\n",
    "# local_specie_classifier = LocalSpecieClassfier(fusion_embedder.fused_dim)\n",
    "\n",
    "# main_classifier = MainClassifier(mat['species2genus'], genus_species, dna_encoder, img_encoder, fusion_embedder, genus_classifier, local_specie_classifier).to(device)\n",
    "# print(main_classifier(**multimodal_collator([dataset[0], dataset[1]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8af25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenusPredictor(\n",
       "  (fusion_encoder): AttentionFusion(\n",
       "    (dna_len_emb): Embedding(120, 16)\n",
       "    (proj_dna): Linear(in_features=512, out_features=496, bias=True)\n",
       "    (proj_img): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (ffn): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (ffn): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=744, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Linear(in_features=744, out_features=372, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genus_predictor.load_state_dict(torch.load('output/Tue Aug 12 20:36:21 2025_best_genus_predictor.pt'))\n",
    "genus_predictor.to(deviceCPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "reload(models)\n",
    "from models import AttentionFusion, MainClassifier, Decoder, GenusClassifier\n",
    "\n",
    "specie_predictor = MainClassifier(mat['species2genus'],genus_species, genus_predictor).to(deviceCPU)\n",
    "specie_predictor.fit(\n",
    "    all_dna_len_tokens,\n",
    "    all_dna_features_cnn_pca, \n",
    "    all_image_features_gan_pca, \n",
    "    mat['all_labels'].squeeze(), \n",
    "    mat['val_seen_loc'].squeeze(), \n",
    "    mat['train_loc'].squeeze(), \n",
    "    200, \n",
    "    lr=0.005,\n",
    "    eval_frequency=10,\n",
    "    freeze_genus=True,\n",
    "    teacher_force=True,\n",
    "    device=deviceCPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f6ae599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32424, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dna_features_cnn_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902101d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    2,    3, ..., 1048, 1049, 1050], shape=(1050,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mat['all_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4612702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNAImageDecoder(nn.Module):\n",
    "    def __init__(self, N_dna, N_image, d_model=128, num_heads=4, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Project DNA and image embeddings into same space\n",
    "        self.dna_proj = nn.Linear(N_dna, d_model)\n",
    "        self.img_proj = nn.Linear(N_image, d_model)\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Feed-forward layer after attention\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, dna_emb, img_emb):\n",
    "        \"\"\"\n",
    "        dna_emb: (batch_size, N_dna)\n",
    "        img_emb: (batch_size, N_image)\n",
    "        \"\"\"\n",
    "        # Project to same dimension\n",
    "        dna_token = self.dna_proj(dna_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "        img_token = self.img_proj(img_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "        \n",
    "        # Sequence: [DNA, Image]\n",
    "        seq = torch.cat([dna_token, img_token], dim=1)  # (batch, 2, d_model)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attn(seq, seq, seq)  # (batch, 2, d_model)\n",
    "        \n",
    "        # Pooling — use first token (DNA) or mean-pool\n",
    "        pooled = attn_out.mean(dim=1)  # (batch, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.ffn(pooled)  # (batch, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ma = nn.MultiheadAttention(768, 4)\n",
    "# attn_output, attn_output_weights = (ma(torch.rand(1, 1, 768), torch.rand(1, 1, 768), torch.rand(1, 1, 768)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b67d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d50c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "# dna_emb = dnaencoder.model(**dnaencoder.tokenizer(dna, return_tensors='pt',))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoImageProcessor, AutoModel\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "\n",
    "# # Load pretrained ViT model\n",
    "# model_name = \"google/vit-base-patch16-224\"\n",
    "# imageprocessor = AutoImageProcessor.from_pretrained(model_name)\n",
    "# vitmodel = AutoModel.from_pretrained(model_name)\n",
    "# vitmodel.eval()\n",
    "\n",
    "# # Put model on GPU if available\n",
    "# deviceCPU = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# vitmodel.to(deviceCPU)\n",
    "\n",
    "# def get_vit_embedding(image_np, model_name=\"google/vit-base-patch16-224\", device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "#     inputs = imageprocessor(images=image_np, return_tensors=\"pt\")\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = vitmodel(**inputs)\n",
    "    \n",
    "#     return outputs.last_hidden_state[:, 0]  # CLS token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71924dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
