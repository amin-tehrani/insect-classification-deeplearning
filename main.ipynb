{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58c9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amintehrani/.pyenv/versions/3.12.8/envs/torchgeo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchviz\n",
    "import dnabert\n",
    "import vit\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataset\n",
    "import dnabert\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6b6392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'all_images', 'all_dnas', 'all_labels', 'all_dnas_norepeat', 'all_dna_labels_norepeat', 'all_boldids', 'train_loc', 'val_seen_loc', 'val_unseen_loc', 'test_seen_loc', 'test_unseen_loc', 'species2genus', 'described_species_labels_train', 'described_species_labels_trainval', 'all_dna_features_cnn_original', 'all_image_features_resnet', 'all_image_features_gan', 'all_dna_features_cnn_new', 'all_string_dnas'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAT_FILE_PATH = './insect_dataset.mat'\n",
    "\n",
    "mat = scipy.io.loadmat(MAT_FILE_PATH)\n",
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d8d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "Max specie in genus:  23\n"
     ]
    }
   ],
   "source": [
    "species2genus = mat['species2genus']-1\n",
    "\n",
    "# group species by genus\n",
    "\n",
    "genus_species = dict()\n",
    "max_specie_in_genus = 0\n",
    "for genus_id, genus in pd.DataFrame(species2genus, columns=['genus']).groupby('genus'):\n",
    "    specie_indices = genus.index.tolist()\n",
    "    genus_species[genus_id] = specie_indices\n",
    "    if len(specie_indices) > max_specie_in_genus:\n",
    "        max_specie_in_genus = len(specie_indices)\n",
    "\n",
    "print(len(genus_species))\n",
    "print(\"Max specie in genus: \", max_specie_in_genus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71eba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# genus = lambda s: mat['species2genus'][s]-1\n",
    "# genus(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a3b7f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038    759\n",
       "977     540\n",
       "1039    361\n",
       "418     292\n",
       "979     292\n",
       "       ... \n",
       "529       4\n",
       "578       2\n",
       "530       2\n",
       "533       2\n",
       "156       1\n",
       "Name: count, Length: 1050, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group labels count\n",
    "pd.Series(mat['all_labels'].squeeze()).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db67da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=512)\n",
    "all_dna_features_cnn_pca = pca.fit_transform(mat['all_dna_features_cnn_new'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23faf696",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=512)\n",
    "all_image_features_gan_pca = pca.fit_transform(mat['all_image_features_gan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c729c15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat['species2genus'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513ccdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13039]) torch.Size([13039]) torch.Size([13039, 23])\n",
      "torch.Size([13039]) torch.Size([13039]) torch.Size([13039, 23])\n",
      "torch.Size([3234]) torch.Size([3234]) torch.Size([3234, 23])\n",
      "torch.Size([13039]) torch.Size([13039]) torch.Size([13039, 23])\n",
      "torch.Size([13039]) torch.Size([13039]) torch.Size([13039, 23])\n",
      "torch.Size([3234]) torch.Size([3234]) torch.Size([3234, 23])\n",
      "torch.Size([13039]) torch.Size([13039]) torch.Size([13039, 23])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [15.227043151855469,\n",
       "  15.225469589233398,\n",
       "  15.198355674743652,\n",
       "  15.172632217407227,\n",
       "  15.160755157470703],\n",
       " 'val_loss': [15.190596580505371, 15.153709411621094],\n",
       " 'val_accuracy': [0.0, 0.0],\n",
       " 'val_ari': [0.0, 0.0],\n",
       " 'val_nmi': [0.0, 0.0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models\n",
    "reload(models)\n",
    "from models import AttentionFusion, SpeciePredictor, Decoder, GenusPredictor\n",
    "device = torch.device('cpu')\n",
    "fusion = AttentionFusion(512, 512, 512).to(device)\n",
    "genus_predictor = GenusPredictor(fusion).to(device)\n",
    "specie_predictor = SpeciePredictor(mat['species2genus'],genus_species, genus_predictor).to(device)\n",
    "specie_predictor.fit(all_dna_features_cnn_pca, \n",
    "              all_image_features_gan_pca, \n",
    "              mat['all_labels'].squeeze(), \n",
    "              mat['val_seen_loc'].squeeze(), \n",
    "              mat['train_loc'].squeeze(), \n",
    "              5, \n",
    "              lr=0.0005,\n",
    "              eval_frequency=2,\n",
    "              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f6ae599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32424, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dna_features_cnn_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902101d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    2,    3, ..., 1048, 1049, 1050], shape=(1050,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mat['all_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4612702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNAImageDecoder(nn.Module):\n",
    "    def __init__(self, N_dna, N_image, d_model=128, num_heads=4, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Project DNA and image embeddings into same space\n",
    "        self.dna_proj = nn.Linear(N_dna, d_model)\n",
    "        self.img_proj = nn.Linear(N_image, d_model)\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Feed-forward layer after attention\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, dna_emb, img_emb):\n",
    "        \"\"\"\n",
    "        dna_emb: (batch_size, N_dna)\n",
    "        img_emb: (batch_size, N_image)\n",
    "        \"\"\"\n",
    "        # Project to same dimension\n",
    "        dna_token = self.dna_proj(dna_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "        img_token = self.img_proj(img_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "        \n",
    "        # Sequence: [DNA, Image]\n",
    "        seq = torch.cat([dna_token, img_token], dim=1)  # (batch, 2, d_model)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attn(seq, seq, seq)  # (batch, 2, d_model)\n",
    "        \n",
    "        # Pooling â€” use first token (DNA) or mean-pool\n",
    "        pooled = attn_out.mean(dim=1)  # (batch, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.ffn(pooled)  # (batch, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3cbcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = nn.MultiheadAttention(768, 4)\n",
    "attn_output, attn_output_weights = (ma(torch.rand(1, 1, 768), torch.rand(1, 1, 768), torch.rand(1, 1, 768)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01b67d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d50c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "dna_emb = dnabert.model(**dnabert.tokenizer(dna, return_tensors='pt',))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0214d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load pretrained ViT model\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "imageprocessor = AutoImageProcessor.from_pretrained(model_name)\n",
    "vitmodel = AutoModel.from_pretrained(model_name)\n",
    "vitmodel.eval()\n",
    "\n",
    "# Put model on GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vitmodel.to(device)\n",
    "\n",
    "def get_vit_embedding(image_np, model_name=\"google/vit-base-patch16-224\", device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    inputs = imageprocessor(images=image_np, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = vitmodel(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state[:, 0]  # CLS token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b741af4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_emb.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71924dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
